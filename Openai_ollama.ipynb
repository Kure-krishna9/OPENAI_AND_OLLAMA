{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f3f8c08",
   "metadata": {},
   "source": [
    "## Getting started with Langchain and Open AI\n",
    "in the quickstart we'll see how to:\n",
    "#### Get set up with LangChain ,LangSmith and LangServe.\n",
    "#### Use the most commen component of LangChain:Promts,Models,and output parser.\n",
    "####    Build simple application with LangChain.\n",
    "####    Trace youre application with LangSmith.\n",
    "####    Serve youre application with LangServe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "594ffb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 1)) (0.3.27)\n",
      "Requirement already satisfied: ipykernel in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 2)) (6.30.1)\n",
      "Requirement already satisfied: python-dotenv in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 3)) (1.1.1)\n",
      "Requirement already satisfied: langchain_community in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 4)) (0.3.29)\n",
      "Requirement already satisfied: pypdf in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 5)) (6.0.0)\n",
      "Requirement already satisfied: bs4 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 6)) (0.0.2)\n",
      "Requirement already satisfied: arxiv in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 7)) (2.2.0)\n",
      "Requirement already satisfied: pymupdf in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 8)) (1.26.4)\n",
      "Requirement already satisfied: wikipedia in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 9)) (1.4.0)\n",
      "Requirement already satisfied: langchain-text-splitters in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 10)) (0.3.11)\n",
      "Requirement already satisfied: langchain-openai in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 11)) (0.3.32)\n",
      "Requirement already satisfied: chromadb in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 12)) (1.0.20)\n",
      "Requirement already satisfied: sentence_transformers in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 13)) (5.1.0)\n",
      "Requirement already satisfied: langchain_huggingface in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 14)) (0.3.1)\n",
      "Requirement already satisfied: faiss-cpu in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 15)) (1.12.0)\n",
      "Requirement already satisfied: langchain_chroma in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 17)) (0.2.5)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain->-r requirement.txt (line 1)) (0.3.75)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain->-r requirement.txt (line 1)) (0.4.21)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain->-r requirement.txt (line 1)) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain->-r requirement.txt (line 1)) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain->-r requirement.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain->-r requirement.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain->-r requirement.txt (line 1)) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain->-r requirement.txt (line 1)) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain->-r requirement.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain->-r requirement.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain->-r requirement.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirement.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirement.txt (line 1)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirement.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from requests<3,>=2->langchain->-r requirement.txt (line 1)) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from requests<3,>=2->langchain->-r requirement.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from requests<3,>=2->langchain->-r requirement.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from requests<3,>=2->langchain->-r requirement.txt (line 1)) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirement.txt (line 1)) (3.2.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (1.8.16)\n",
      "Requirement already satisfied: ipython>=7.23.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (9.5.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=25 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (27.0.2)\n",
      "Requirement already satisfied: tornado>=6.2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (5.14.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain_community->-r requirement.txt (line 4)) (3.12.15)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain_community->-r requirement.txt (line 4)) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain_community->-r requirement.txt (line 4)) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain_community->-r requirement.txt (line 4)) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain_community->-r requirement.txt (line 4)) (2.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirement.txt (line 4)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirement.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirement.txt (line 4)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirement.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirement.txt (line 4)) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirement.txt (line 4)) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirement.txt (line 4)) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community->-r requirement.txt (line 4)) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community->-r requirement.txt (line 4)) (0.9.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain_community->-r requirement.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from bs4->-r requirement.txt (line 6)) (4.13.5)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from arxiv->-r requirement.txt (line 7)) (6.0.11)\n",
      "Requirement already satisfied: sgmllib3k in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from feedparser~=6.0.10->arxiv->-r requirement.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.99.9 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain-openai->-r requirement.txt (line 11)) (1.102.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain-openai->-r requirement.txt (line 11)) (0.11.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.99.9->langchain-openai->-r requirement.txt (line 11)) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.99.9->langchain-openai->-r requirement.txt (line 11)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.99.9->langchain-openai->-r requirement.txt (line 11)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.99.9->langchain-openai->-r requirement.txt (line 11)) (0.10.0)\n",
      "Requirement already satisfied: sniffio in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.99.9->langchain-openai->-r requirement.txt (line 11)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.99.9->langchain-openai->-r requirement.txt (line 11)) (4.67.1)\n",
      "Requirement already satisfied: httpcore==1.* in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain-openai->-r requirement.txt (line 11)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain-openai->-r requirement.txt (line 11)) (0.16.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai->-r requirement.txt (line 11)) (2025.8.29)\n",
      "Requirement already satisfied: build>=1.0.3 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirement.txt (line 12)) (0.35.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (1.22.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (1.36.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (0.22.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (1.74.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (0.17.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (3.11.3)\n",
      "Requirement already satisfied: rich>=10.11.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (14.1.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (4.25.1)\n",
      "Requirement already satisfied: six>=1.5 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r requirement.txt (line 12)) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r requirement.txt (line 12)) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r requirement.txt (line 12)) (2.2.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from sentence_transformers->-r requirement.txt (line 13)) (4.56.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from sentence_transformers->-r requirement.txt (line 13)) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from sentence_transformers->-r requirement.txt (line 13)) (1.7.1)\n",
      "Requirement already satisfied: scipy in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from sentence_transformers->-r requirement.txt (line 13)) (1.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from sentence_transformers->-r requirement.txt (line 13)) (0.34.4)\n",
      "Requirement already satisfied: Pillow in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from sentence_transformers->-r requirement.txt (line 13)) (11.3.0)\n",
      "Requirement already satisfied: filelock in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers->-r requirement.txt (line 13)) (3.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers->-r requirement.txt (line 13)) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers->-r requirement.txt (line 13)) (2025.7.0)\n",
      "Requirement already satisfied: pyproject_hooks in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb->-r requirement.txt (line 12)) (1.2.0)\n",
      "Requirement already satisfied: colorama in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb->-r requirement.txt (line 12)) (0.4.6)\n",
      "Requirement already satisfied: decorator in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (2.19.2)\n",
      "Requirement already satisfied: stack_data in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (0.8.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb->-r requirement.txt (line 12)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb->-r requirement.txt (line 12)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb->-r requirement.txt (line 12)) (0.27.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirement.txt (line 2)) (4.4.0)\n",
      "Requirement already satisfied: pywin32>=300 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirement.txt (line 2)) (311)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 12)) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 12)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 12)) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 12)) (3.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 12)) (0.10)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirement.txt (line 12)) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirement.txt (line 12)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirement.txt (line 12)) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirement.txt (line 12)) (0.6.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain->-r requirement.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain->-r requirement.txt (line 1)) (0.24.0)\n",
      "Requirement already satisfied: coloredlogs in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 12)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 12)) (25.2.10)\n",
      "Requirement already satisfied: protobuf in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 12)) (6.32.0)\n",
      "Requirement already satisfied: sympy in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 12)) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb->-r requirement.txt (line 12)) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r requirement.txt (line 12)) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirement.txt (line 12)) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirement.txt (line 12)) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirement.txt (line 12)) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb->-r requirement.txt (line 12)) (0.57b0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from rich>=10.11.0->chromadb->-r requirement.txt (line 12)) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->-r requirement.txt (line 12)) (0.1.2)\n",
      "Requirement already satisfied: networkx in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers->-r requirement.txt (line 13)) (3.5)\n",
      "Requirement already satisfied: jinja2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers->-r requirement.txt (line 13)) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 12)) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from typer>=0.9.0->chromadb->-r requirement.txt (line 12)) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from typer>=0.9.0->chromadb->-r requirement.txt (line 12)) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirement.txt (line 12)) (0.6.4)\n",
      "Requirement already satisfied: watchfiles>=0.13 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirement.txt (line 12)) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirement.txt (line 12)) (15.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from beautifulsoup4->bs4->-r requirement.txt (line 6)) (2.8)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 12)) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 12)) (3.5.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers->-r requirement.txt (line 13)) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from scikit-learn->sentence_transformers->-r requirement.txt (line 13)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from scikit-learn->sentence_transformers->-r requirement.txt (line 13)) (3.6.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirement.txt\n",
    "# !python -m venv myenv\n",
    "# !myenv/Scripts/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e0346c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"Open_API_Key\")\n",
    "## Lang smith tracking\n",
    "os.environ[\"Langchain_api_key\"]=os.getenv(\"Langchain_api_key\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"Langchain_project\"]=os.getenv(\"Langchain_project\")\n",
    "os.environ[\"HF_token\"]=os.getenv(\"HF_token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771bc146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f344c13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000002D147BC62D0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002D147C9ED90> root_client=<openai.OpenAI object at 0x000002D147775CD0> root_async_client=<openai.AsyncOpenAI object at 0x000002D147C9E890> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12421482",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m### input and get response from llm\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhat is gen ai?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:393\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    383\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    388\u001b[39m     **kwargs: Any,\n\u001b[32m    389\u001b[39m ) -> BaseMessage:\n\u001b[32m    390\u001b[39m     config = ensure_config(config)\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    392\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    403\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1019\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1010\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1012\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1016\u001b[39m     **kwargs: Any,\n\u001b[32m   1017\u001b[39m ) -> LLMResult:\n\u001b[32m   1018\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1019\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:837\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    834\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    835\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    836\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    843\u001b[39m         )\n\u001b[32m    844\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    845\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1085\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1083\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1084\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1085\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1089\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1183\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1181\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1182\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1183\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1185\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1186\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1187\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1188\u001b[39m ):\n\u001b[32m   1189\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1178\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1171\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1172\u001b[39m             response,\n\u001b[32m   1173\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1174\u001b[39m             metadata=generation_info,\n\u001b[32m   1175\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1176\u001b[39m         )\n\u001b[32m   1177\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1178\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1179\u001b[39m         response = raw_response.parse()\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "### input and get response from llm\n",
    "result=llm.invoke(\"what is gen ai?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e4e16f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresult\u001b[49m.content)\n",
      "\u001b[31mNameError\u001b[39m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a53d5d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are expect AI engineer.Provide me Answers based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chat promt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"you are expect AI engineer.Provide me Answers based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a9aad6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m chain=prompt|llm\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m result=\u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCan you tell me about Langsmith\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3082\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3080\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3081\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3082\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3084\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:393\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    383\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    388\u001b[39m     **kwargs: Any,\n\u001b[32m    389\u001b[39m ) -> BaseMessage:\n\u001b[32m    390\u001b[39m     config = ensure_config(config)\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    392\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    403\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1019\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1010\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1012\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1016\u001b[39m     **kwargs: Any,\n\u001b[32m   1017\u001b[39m ) -> LLMResult:\n\u001b[32m   1018\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1019\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:837\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    834\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    835\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    836\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    843\u001b[39m         )\n\u001b[32m    844\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    845\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1085\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1083\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1084\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1085\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1089\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1183\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1181\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1182\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1183\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1185\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1186\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1187\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1188\u001b[39m ):\n\u001b[32m   1189\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1178\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1171\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1172\u001b[39m             response,\n\u001b[32m   1173\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1174\u001b[39m             metadata=generation_info,\n\u001b[32m   1175\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1176\u001b[39m         )\n\u001b[32m   1177\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1178\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1179\u001b[39m         response = raw_response.parse()\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "## Chain\n",
    "chain=prompt|llm\n",
    "result=chain.invoke({\"input\":\"Can you tell me about Langsmith\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdb49784",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m output_parser=StrOutputParser()\n\u001b[32m      4\u001b[39m chain=prompt|llm|output_parser\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m response=\u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCan you tell me about Langsmith\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3082\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3080\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3081\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3082\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3084\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:393\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    383\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    388\u001b[39m     **kwargs: Any,\n\u001b[32m    389\u001b[39m ) -> BaseMessage:\n\u001b[32m    390\u001b[39m     config = ensure_config(config)\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    392\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    403\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1019\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1010\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1012\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1016\u001b[39m     **kwargs: Any,\n\u001b[32m   1017\u001b[39m ) -> LLMResult:\n\u001b[32m   1018\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1019\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:837\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    834\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    835\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    836\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    843\u001b[39m         )\n\u001b[32m    844\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    845\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1085\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1083\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1084\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1085\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1089\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1183\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1181\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1182\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1183\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1185\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1186\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1187\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1188\u001b[39m ):\n\u001b[32m   1189\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1178\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1171\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1172\u001b[39m             response,\n\u001b[32m   1173\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1174\u001b[39m             metadata=generation_info,\n\u001b[32m   1175\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1176\u001b[39m         )\n\u001b[32m   1177\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1178\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1179\u001b[39m         response = raw_response.parse()\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "## String Output parser \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1910d6ac",
   "metadata": {},
   "source": [
    "## Simple GENAI APPlication with Langchain Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94a0227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"Open_API_Key\")\n",
    "## Lang smith tracking\n",
    "os.environ[\"Langchain_api_key\"]=os.getenv(\"Langchain_api_key\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"Langchain_project\"]=os.getenv(\"Langchain_project\")\n",
    "os.environ[\"HF_token\"]=os.getenv(\"HF_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7071a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x2d147cc13d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vData ingetion data from the wbsite we need to scrapt the data\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53697324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Get started with LangSmith - Docs by LangChainDocs by LangChain home pagePythonSearch...KLangSmithPlatform for LLM observability and evaluationOverviewQuickstartsTrace an applicationEvaluate an applicationTest promptsAPI & SDKsAPI referencePython SDKJS/TS SDKPricingPlansPricing FAQDocs by LangChain home pagePythonSearch...KAsk AIGitHubForumForumSearch...NavigationGet started with LangSmithGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumGet started with LangSmithCopy pageCopy pageLangSmith is a platform for building production-grade LLM applications. Monitor and evaluate your application, so you can ship quickly and with confidence.\\nLangSmith is framework agnostic \\xa0you can use it with or without LangChains open source frameworks langchain and langgraph.\\n\\n\\nStart tracingGain visibility into each step your application takes when handling a request to debug faster.Learn moreEvaluate your applicationMeasure quality of your applications over time to build more reliable AI applications.Learn moreTest your promptsIterate on prompts, with automatic version control and collaboration features.Learn moreSet up your workspaceSet up your workspace, configure admin settings, and invite your team to collaborate.Learn moreTrace an applicationAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=loader.load()\n",
    "\n",
    "# docs[0].page_content\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "209bb934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='Get started with LangSmith - Docs by LangChainDocs'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='by LangChainDocs by LangChain home'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='home pagePythonSearch...KLangSmithPlatform for'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='for LLM observability and'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='and evaluationOverviewQuickstartsTrace an'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='an applicationEvaluate an applicationTest'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='an applicationTest promptsAPI & SDKsAPI'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='& SDKsAPI referencePython SDKJS/TS'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='SDKJS/TS SDKPricingPlansPricing FAQDocs by'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='FAQDocs by LangChain home'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='by LangChain home pagePythonSearch...KAsk'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='AIGitHubForumForumSearch...NavigationGet started'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='started with LangSmithGet'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='LangSmithGet startedObservabilityEvaluationPrompt'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='engineeringSelf-hostingAdministrationGet'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='startedObservabilityEvaluationPrompt'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='engineeringSelf-hostingAdministrationGitHubForumG'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='strationGitHubForumGet'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='started with LangSmithCopy pageCopy pageLangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='pageLangSmith is a platform for building'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='for building production-grade LLM applications.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='LLM applications. Monitor and evaluate your'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='and evaluate your application, so you can ship'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='so you can ship quickly and with confidence.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='LangSmith is framework agnostic \\xa0you can use it'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='\\xa0you can use it with or without LangChains open'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='LangChains open source frameworks langchain and'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='langchain and langgraph.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='Start tracingGain visibility into each step your'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='into each step your application takes when'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='takes when handling a request to debug'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='a request to debug faster.Learn moreEvaluate your'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='moreEvaluate your applicationMeasure quality of'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='quality of your applications over time to build'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='over time to build more reliable AI'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='more reliable AI applications.Learn moreTest your'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='moreTest your promptsIterate on prompts, with'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='on prompts, with automatic version control and'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='version control and collaboration features.Learn'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='features.Learn moreSet up your workspaceSet up'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='workspaceSet up your workspace, configure admin'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='configure admin settings, and invite your team to'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='invite your team to collaborate.Learn moreTrace'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='moreTrace an applicationAssistantResponses are'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='are generated using AI and may contain'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='AI and may contain mistakes.Docs by LangChain'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='by LangChain home'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='pagegithubxlinkedinyoutubeResourcesChangelogLangC'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='ourcesChangelogLangChain'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='AcademyTrust'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='CenterCompanyAboutCareersBloggithubxlinkedinyoutu'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='githubxlinkedinyoutubePowered'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith - Docs by LangChain', 'language': 'en'}, page_content='by Mintlify')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load Data -->Divided our text into chunk---->text---->vectors---->Vectore Embeddings----Vectore store db\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=50,chunk_overlap=20)\n",
    "documents=text_splitter.split_documents(docs)\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34cae956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it into vectore  using embeding techniques\n",
    "\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb7c389a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# store vetore in Vectore store db\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m vectorestore_db=\u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:848\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    846\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1043\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_texts\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m     **kwargs: Any,\n\u001b[32m   1024\u001b[39m ) -> FAISS:\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m     embeddings = \u001b[43membedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.__from(\n\u001b[32m   1045\u001b[39m         texts,\n\u001b[32m   1046\u001b[39m         embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1050\u001b[39m         **kwargs,\n\u001b[32m   1051\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:591\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    588\u001b[39m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[32m    589\u001b[39m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[32m    590\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:479\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    477\u001b[39m batched_embeddings: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]] = []\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    482\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    483\u001b[39m         response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# store vetore in Vectore store db\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorestore_db=FAISS.from_documents(documents,embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b4ada6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorestore_db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m query=\u001b[33m\"\u001b[39m\u001b[33m AI applications involve writing prompts to instruct the LLM on what to do.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m result=\u001b[43mvectorestore_db\u001b[49m.similarity_search(query)\n\u001b[32m      3\u001b[39m result[\u001b[32m0\u001b[39m].page_content\n",
      "\u001b[31mNameError\u001b[39m: name 'vectorestore_db' is not defined"
     ]
    }
   ],
   "source": [
    "## Query from a vector db\n",
    "query=\" AI applications involve writing prompts to instruct the LLM on what to do.\"\n",
    "result=vectorestore_db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c4dcb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnesr the following Question based only on the provided Context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002D154182AD0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002D1541B4F90>, root_client=<openai.OpenAI object at 0x000002D1541755D0>, root_async_client=<openai.AsyncOpenAI object at 0x000002D1541B6B50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Retrival cahin\n",
    "## Document Chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "\n",
    "\"\"\"\n",
    "Anesr the following Question based only on the provided Context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "document_chain=create_stuff_documents_chain(llm=llm ,prompt=prompt )\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "687bb497",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocuments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdocument_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m AI applications involve writing prompts to instruct the LLM on what to do.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDocument\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_content\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m AI applications involve writing prompts to instruct the LLM on what to do.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5495\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5488\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5489\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5490\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5493\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5494\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5495\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5496\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5497\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5498\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3082\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3080\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3081\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3082\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3084\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:393\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    383\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    388\u001b[39m     **kwargs: Any,\n\u001b[32m    389\u001b[39m ) -> BaseMessage:\n\u001b[32m    390\u001b[39m     config = ensure_config(config)\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    392\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    403\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1019\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1010\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1012\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1016\u001b[39m     **kwargs: Any,\n\u001b[32m   1017\u001b[39m ) -> LLMResult:\n\u001b[32m   1018\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1019\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:837\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    834\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    835\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    836\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    843\u001b[39m         )\n\u001b[32m    844\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    845\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1085\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1083\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1084\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1085\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1089\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1183\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1181\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1182\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1183\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1185\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1186\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1187\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1188\u001b[39m ):\n\u001b[32m   1189\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1178\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1171\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1172\u001b[39m             response,\n\u001b[32m   1173\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1174\u001b[39m             metadata=generation_info,\n\u001b[32m   1175\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1176\u001b[39m         )\n\u001b[32m   1177\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1178\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1179\u001b[39m         response = raw_response.parse()\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\study material\\NLP_Rag _Projects\\OPENAI_AND_OLLAMA\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":\" AI applications involve writing prompts to instruct the LLM on what to do.\",\n",
    "    \"context\":[Document(page_content=\" AI applications involve writing prompts to instruct the LLM on what to do.\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c708e6",
   "metadata": {},
   "source": [
    "however we want the document to first come from the retrivelr we just set up. that whay,we can use the retiver to dynamically select the most relevent document and pass those in for a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1745ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### input-->Retriver---->Vectorestoredb\n",
    "vectorestore_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b672e8d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorestore_db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# creata a Retiver\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m retriver=\u001b[43mvectorestore_db\u001b[49m.as_retriever()\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_retrieval_chain\n\u001b[32m      4\u001b[39m create_retrieval_chain(retriver,document_chain)\n",
      "\u001b[31mNameError\u001b[39m: name 'vectorestore_db' is not defined"
     ]
    }
   ],
   "source": [
    "# creata a Retiver\n",
    "retriver=vectorestore_db.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retriver_chain=create_retrieval_chain(retriver,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72ac456f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriver_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mretriver_chain\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'retriver_chain' is not defined"
     ]
    }
   ],
   "source": [
    "retriver_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a247c80b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriver_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Get the response from llm\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response=\u001b[43mretriver_chain\u001b[49m.invoke({\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33m AI applications involve writing prompts to instruct the LLM on what to do.\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m      4\u001b[39m response[\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'retriver_chain' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the response from llm\n",
    "response=retriver_chain.invoke({\"input\":\" AI applications involve writing prompts to instruct the LLM on what to do.\"})\n",
    "\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31fc0436",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mresponse\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "response['context']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c0620",
   "metadata": {},
   "source": [
    "## Simple GENAI APPlication with Langchain Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f393e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 1)) (0.3.27)\n",
      "Requirement already satisfied: ipykernel in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 2)) (6.30.1)\n",
      "Requirement already satisfied: python-dotenv in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 3)) (1.1.1)\n",
      "Requirement already satisfied: langchain_community in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 4)) (0.3.29)\n",
      "Requirement already satisfied: pypdf in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 5)) (6.0.0)\n",
      "Requirement already satisfied: bs4 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 6)) (0.0.2)\n",
      "Requirement already satisfied: arxiv in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 7)) (2.2.0)\n",
      "Requirement already satisfied: pymupdf in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 8)) (1.26.4)\n",
      "Requirement already satisfied: wikipedia in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 9)) (1.4.0)\n",
      "Requirement already satisfied: langchain-text-splitters in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 10)) (0.3.11)\n",
      "Requirement already satisfied: langchain-openai in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 11)) (0.3.32)\n",
      "Requirement already satisfied: chromadb in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 12)) (1.0.20)\n",
      "Requirement already satisfied: sentence_transformers in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 13)) (5.1.0)\n",
      "Requirement already satisfied: langchain_huggingface in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 14)) (0.3.1)\n",
      "Requirement already satisfied: faiss-cpu in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 15)) (1.12.0)\n",
      "Requirement already satisfied: langchain_chroma in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 17)) (0.2.5)\n",
      "Requirement already satisfied: streamlit in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from -r requirement.txt (line 18)) (1.49.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain->-r requirement.txt (line 1)) (0.3.75)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain->-r requirement.txt (line 1)) (0.4.21)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain->-r requirement.txt (line 1)) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain->-r requirement.txt (line 1)) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain->-r requirement.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain->-r requirement.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain->-r requirement.txt (line 1)) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain->-r requirement.txt (line 1)) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain->-r requirement.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain->-r requirement.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain->-r requirement.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirement.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirement.txt (line 1)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirement.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from requests<3,>=2->langchain->-r requirement.txt (line 1)) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from requests<3,>=2->langchain->-r requirement.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from requests<3,>=2->langchain->-r requirement.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from requests<3,>=2->langchain->-r requirement.txt (line 1)) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirement.txt (line 1)) (3.2.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (1.8.16)\n",
      "Requirement already satisfied: ipython>=7.23.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (9.5.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=25 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (27.0.2)\n",
      "Requirement already satisfied: tornado>=6.2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipykernel->-r requirement.txt (line 2)) (5.14.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain_community->-r requirement.txt (line 4)) (3.12.15)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain_community->-r requirement.txt (line 4)) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain_community->-r requirement.txt (line 4)) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain_community->-r requirement.txt (line 4)) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain_community->-r requirement.txt (line 4)) (2.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirement.txt (line 4)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirement.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirement.txt (line 4)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirement.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirement.txt (line 4)) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirement.txt (line 4)) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirement.txt (line 4)) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community->-r requirement.txt (line 4)) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community->-r requirement.txt (line 4)) (0.9.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain_community->-r requirement.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from bs4->-r requirement.txt (line 6)) (4.13.5)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from arxiv->-r requirement.txt (line 7)) (6.0.11)\n",
      "Requirement already satisfied: sgmllib3k in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from feedparser~=6.0.10->arxiv->-r requirement.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.99.9 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain-openai->-r requirement.txt (line 11)) (1.102.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langchain-openai->-r requirement.txt (line 11)) (0.11.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.99.9->langchain-openai->-r requirement.txt (line 11)) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.99.9->langchain-openai->-r requirement.txt (line 11)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.99.9->langchain-openai->-r requirement.txt (line 11)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.99.9->langchain-openai->-r requirement.txt (line 11)) (0.10.0)\n",
      "Requirement already satisfied: sniffio in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.99.9->langchain-openai->-r requirement.txt (line 11)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.99.9->langchain-openai->-r requirement.txt (line 11)) (4.67.1)\n",
      "Requirement already satisfied: httpcore==1.* in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain-openai->-r requirement.txt (line 11)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain-openai->-r requirement.txt (line 11)) (0.16.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai->-r requirement.txt (line 11)) (2025.8.29)\n",
      "Requirement already satisfied: build>=1.0.3 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirement.txt (line 12)) (0.35.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (1.22.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (1.36.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (0.22.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (1.74.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (0.17.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (3.11.3)\n",
      "Requirement already satisfied: rich>=10.11.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (14.1.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from chromadb->-r requirement.txt (line 12)) (4.25.1)\n",
      "Requirement already satisfied: six>=1.5 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r requirement.txt (line 12)) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r requirement.txt (line 12)) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r requirement.txt (line 12)) (2.2.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from sentence_transformers->-r requirement.txt (line 13)) (4.56.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from sentence_transformers->-r requirement.txt (line 13)) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from sentence_transformers->-r requirement.txt (line 13)) (1.7.1)\n",
      "Requirement already satisfied: scipy in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from sentence_transformers->-r requirement.txt (line 13)) (1.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from sentence_transformers->-r requirement.txt (line 13)) (0.34.4)\n",
      "Requirement already satisfied: Pillow in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from sentence_transformers->-r requirement.txt (line 13)) (11.3.0)\n",
      "Requirement already satisfied: filelock in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers->-r requirement.txt (line 13)) (3.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers->-r requirement.txt (line 13)) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers->-r requirement.txt (line 13)) (2025.7.0)\n",
      "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from streamlit->-r requirement.txt (line 18)) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from streamlit->-r requirement.txt (line 18)) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from streamlit->-r requirement.txt (line 18)) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from streamlit->-r requirement.txt (line 18)) (8.2.1)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from streamlit->-r requirement.txt (line 18)) (2.3.2)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from streamlit->-r requirement.txt (line 18)) (6.32.0)\n",
      "Requirement already satisfied: pyarrow>=7.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from streamlit->-r requirement.txt (line 18)) (21.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from streamlit->-r requirement.txt (line 18)) (0.10.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from streamlit->-r requirement.txt (line 18)) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from streamlit->-r requirement.txt (line 18)) (3.1.45)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from streamlit->-r requirement.txt (line 18)) (0.9.1)\n",
      "Requirement already satisfied: jinja2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r requirement.txt (line 18)) (3.1.6)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r requirement.txt (line 18)) (2.3.0)\n",
      "Requirement already satisfied: colorama in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from click<9,>=7.0->streamlit->-r requirement.txt (line 18)) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirement.txt (line 18)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirement.txt (line 18)) (5.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit->-r requirement.txt (line 18)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit->-r requirement.txt (line 18)) (2025.2)\n",
      "Requirement already satisfied: pyproject_hooks in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb->-r requirement.txt (line 12)) (1.2.0)\n",
      "Requirement already satisfied: decorator in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (2.19.2)\n",
      "Requirement already satisfied: stack_data in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (0.8.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r requirement.txt (line 18)) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb->-r requirement.txt (line 12)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb->-r requirement.txt (line 12)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb->-r requirement.txt (line 12)) (0.27.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirement.txt (line 2)) (4.4.0)\n",
      "Requirement already satisfied: pywin32>=300 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirement.txt (line 2)) (311)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 12)) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 12)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 12)) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 12)) (3.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 12)) (0.10)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirement.txt (line 12)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirement.txt (line 12)) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirement.txt (line 12)) (0.6.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain->-r requirement.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain->-r requirement.txt (line 1)) (0.24.0)\n",
      "Requirement already satisfied: coloredlogs in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 12)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 12)) (25.2.10)\n",
      "Requirement already satisfied: sympy in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 12)) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb->-r requirement.txt (line 12)) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r requirement.txt (line 12)) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirement.txt (line 12)) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirement.txt (line 12)) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirement.txt (line 12)) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb->-r requirement.txt (line 12)) (0.57b0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from rich>=10.11.0->chromadb->-r requirement.txt (line 12)) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->-r requirement.txt (line 12)) (0.1.2)\n",
      "Requirement already satisfied: networkx in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers->-r requirement.txt (line 13)) (3.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 12)) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from typer>=0.9.0->chromadb->-r requirement.txt (line 12)) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirement.txt (line 12)) (0.6.4)\n",
      "Requirement already satisfied: watchfiles>=0.13 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirement.txt (line 12)) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirement.txt (line 12)) (15.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from beautifulsoup4->bs4->-r requirement.txt (line 6)) (2.8)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 12)) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 12)) (3.5.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from scikit-learn->sentence_transformers->-r requirement.txt (line 13)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from scikit-learn->sentence_transformers->-r requirement.txt (line 13)) (3.6.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in f:\\study material\\nlp_rag _projects\\openai_and_ollama\\.venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirement.txt (line 2)) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "980d5c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"Open_API_Key\")\n",
    "## Lang smith tracking\n",
    "os.environ[\"Langchain_api_key\"]=os.getenv(\"Langchain_api_key\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"Langchain_project\"]=os.getenv(\"Langchain_project\")\n",
    "os.environ[\"HF_token\"]=os.getenv(\"HF_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3afc7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564859c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Promt Template \n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"you are helpful assistant .please respond to the questions asked\"),\n",
    "        (\"user\",\"Question:{question}\")\n",
    "    ]\n",
    ")\n",
    "## Stremlot Framework\n",
    "st.title(\"Langchain Demo With Gemma:2b\")\n",
    "input_text=st.text_input(\"what question you have in  mind?\")\n",
    "\n",
    "## Ollama Llama2 model\n",
    "llm=OllamaLLM(\"gemma:2b\")\n",
    "\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "if input_text:\n",
    "    st.write(chain.invoke({\"question\":input_text}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
